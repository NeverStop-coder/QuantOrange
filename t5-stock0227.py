import os
import streamlit as st
import akshare as ak
import pandas as pd
import numpy as np
import datetime
from sklearn.linear_model import RidgeCV
from sklearn.preprocessing import StandardScaler
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import io
import requests
from sklearn.linear_model import LassoCV
from sklearn.linear_model import ElasticNetCV

# ==========================================
# 1. å…¨å±€é…ç½®ä¸è¯·æ±‚æ‹¦æˆªè¡¥ä¸
# ==========================================
st.set_page_config(page_title="Aè‚¡æ…¢ç‰›å¥åº·æŒ‡æ ‡é‡åŒ–æ¨¡å‹", layout="wide")

REAL_COOKIE = "qgqp_b_id=8aef636eb69282130f7e8f79da8f6e20; st_nvi=5cZX0CEB3Ba439P2f2vn262be; nid18=0211112583013a1150f6ce06028f1406; nid18_create_time=1766415090157; gviem=-NZ2ghisJi5Bmw8khF6La759c; gviem_create_time=1766415090157; websitepoptg_api_time=1772086010018; st_si=34181324749368; st_asi=delete; fullscreengg=1; fullscreengg2=1; wsc_checkuser_ok=1; st_pvi=23050207989242; st_sp=2025-04-04%2021%3A51%3A55; st_inirUrl=https%3A%2F%2Fcn.bing.com%2F; st_sn=2; st_psi=20260226141436855-111000300841-0119960545" 
    # 2. å®šä¹‰å…¨å±€ä¼ªè£…å¤´
def apply_request_patch(cookie_str):
    HEADERS_PATCH = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0 Safari/537.36",
        "Accept": "*/*",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Referer": "https://quote.eastmoney.com/center/grid_list.html",
        "Cookie": cookie_str,
        "Connection": "close"# å¼ºåˆ¶çŸ­è¿æ¥ï¼Œé˜²æ­¢ä¸œè´¢å¯¹é•¿è¿æ¥çš„å®¡è®¡
    }
    # 3. æ‹¦æˆª requests åº“ï¼Œå¼ºåˆ¶æ³¨å…¥ Headerå’Œå…³é—­ SSL éªŒè¯ (éƒ¨åˆ†æ•°æ®æºå¯èƒ½å­˜åœ¨è¯ä¹¦é—®é¢˜)
    _old_get = requests.get
    def new_get(url, **kwargs):
        if any(domain in url for domain in ["eastmoney.com", "akshare", "sina"]):
            kwargs['headers'] = HEADERS_PATCH
            kwargs['verify'] = False # ç»•è¿‡å¯èƒ½çš„ SSL è¯ä¹¦é˜»æ‹¦
        return _old_get(url, **kwargs)
    requests.get = new_get

# ==========================================
# 1. é¡µé¢é…ç½®ä¸å…¨å±€è®¾ç½®
# ==========================================

st.title("ğŸ“ˆ Aè‚¡æ…¢ç‰›å¥åº·åº¦ä¸å®è§‚å¤šå› å­åŠ¨æ€åˆ†æé¢æ¿ (2026 å¢å¼ºç‰ˆ)")
st.markdown("""
æœ¬å·¥ä½œç«™åŸºäº**æ»šåŠ¨å²­å›å½’**ä¸**è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ**ï¼Œç»“åˆå¼€æºå®è§‚/é‡‘èæ•°æ®æºï¼ˆAkShareï¼‰ï¼Œä¸ºæ‚¨æä¾›å¯äº¤äº’çš„Aè‚¡ä¸Šè¯æŒ‡æ•°èµ°åŠ¿é¢„æµ‹ä¸å¥åº·åº¦åˆ†æã€‚
""")

with st.expander("ğŸ”‘ æ•°æ®æºè®¿é—®æˆæƒ", expanded=True):
    st.subheader("ä¸œæ–¹è´¢å¯Œåçˆ¬æ ¡éªŒ")
    user_cookie = st.text_input("ä¸œæ–¹è´¢å¯Œ Cookieï¼Œè¯·è¾“å…¥å®Œæ•´çš„æµè§ˆå™¨ Cookie å­—ç¬¦ä¸²(é»˜è®¤å¯èƒ½ä¸ºé”™):", value=REAL_COOKIE, type="password",help="è¯·ä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·F12ä¸­è·å–ä¸œè´¢çš„å®Œæ•´ Cookie,1æ‰“å¼€æµè§ˆå™¨ï¼ˆChrome/Edgeï¼‰ï¼Œç™»å½•ä¸œæ–¹è´¢å¯Œç½‘ï¼ˆæˆ–ç›´æ¥æ‰“å¼€è‚¡å§/è¡Œæƒ…é¡µé¢ï¼‰ã€‚2æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·ï¼Œåˆ‡æ¢åˆ° Network (ç½‘ç»œ) æ ‡ç­¾ã€‚3åˆ·æ–°é¡µé¢ï¼Œéšä¾¿æ‰¾ä¸€ä¸ªè¯·æ±‚ï¼Œåœ¨ Request Headers ä¸­æ‰¾åˆ° Cookie è¿™ä¸€é¡¹ã€‚4å¤åˆ¶é‚£ä¸€é•¿ä¸²å­—ç¬¦ä¸²ã€‚")
    apply_request_patch(user_cookie)
    st.write("âœ… å·²åº”ç”¨è¯·æ±‚è¡¥ä¸ï¼Œæ­£åœ¨éªŒè¯æ•°æ®æºè®¿é—®...")

# ==========================================
# 2. æ ¸å¿ƒæ•°æ®è·å–ä¸æ¸…æ´—Pipeline
# ==========================================
@st.cache_data(ttl=3600)
def fetch_and_align_data(start_date, end_date):
    with st.status("ğŸš€ æ­£åœ¨è°ƒåº¦7å¤§å›å½’å› å­ä¸å±•ç¤ºæ•°æ®...", expanded=True) as status:
        try:

            # ç¯å¢ƒæ¸…ç†ï¼Œé˜²æ­¢ä»£ç†å¹²æ‰°
            os.environ['NO_PROXY'] = '*'

            # 1. è‚¡å¸‚åŸºå‡† (æ”¶ç›˜ä»·, æˆäº¤é‡, æ³¢åŠ¨ç‡)
            st.write("ğŸ” æ­£åœ¨è¿æ¥ï¼šä¸œæ–¹è´¢å¯ŒæœåŠ¡å™¨ (AkShare è¡Œæƒ…)...")
            df_index = ak.stock_zh_index_daily_em(symbol="sh000001")
            st.success("âœ… ä¸œæ–¹è´¢å¯Œè¿æ¥æˆåŠŸ")
            df_index['date'] = pd.to_datetime(df_index['date'])
            df_index.set_index('date', inplace=True)
            df_index = df_index.loc[~df_index.index.duplicated(keep='last')]
            df_index.sort_index(inplace=True)

            df_index = df_index[['close', 'volume']].rename(columns={'close': 'Close'})
            df_index['Return'] = df_index['Close'].pct_change()
            df_index['Vol_20d'] = df_index['Return'].rolling(20).std()

            # 2. ç¾å€º & ä¸­å€º -> è®¡ç®—åˆ©å·®
            # ç”±äºè¿‘æœŸç¾å€ºæ•°æ®æ¥å£ä¸ç¨³å®šï¼Œå¢åŠ å¼‚å¸¸å¤„ç†é€»è¾‘ï¼Œå›é€€è‡³æ¨¡æ‹Ÿå€¼æˆ–æ‰‹åŠ¨è¾“å…¥
            try:
                st.write("ğŸ” æ­£åœ¨è¿æ¥ï¼šæ–°æµªç¾å€ºæ•°æ®æº...")
                us_bond = ak.bond_gb_us_sina(symbol="ç¾å›½10å¹´æœŸå›½å€º")
                st.success("âœ… ç¾å€ºæ•°æ®è·å–æˆåŠŸ")
                us_bond['date'] = pd.to_datetime(us_bond['date'])
                us_bond = us_bond.set_index('date')[['close']].rename(columns={'close': 'DGS10'})
                us_bond = us_bond.loc[~us_bond.index.duplicated(keep='last')]
                us_bond.sort_index(inplace=True)

            except Exception as e:
                st.error(f"âš ï¸ ç¾å€ºæ•°æ®è·å–å¤±è´¥: {e}")
                st.warning(f"ç¾å€ºæ¥å£å¾®è°ƒ ({e})ï¼Œå¯ç”¨ 4.05% æ¨¡æ‹Ÿå€¼2026/02/26ï¼Œæˆ–ä½¿ç”¨æ‰‹åŠ¨ä¿®æ”¹")
                df_index['DGS10'] = 4.05
            
            # åŒæ ·å¢åŠ ä¸­å€ºæ•°æ®çš„å¼‚å¸¸å¤„ç†ï¼Œå›é€€è‡³æ¨¡æ‹Ÿå€¼æˆ–æ‰‹åŠ¨è¾“å…¥
            try:
                st.write("ğŸ” æ­£åœ¨è¿æ¥ï¼šæ–°æµªå›½å€ºæ•°æ®æº...")
                cn_bond = ak.bond_gb_zh_sina(symbol="ä¸­å›½10å¹´æœŸå›½å€º")
                st.success("âœ… å›½å€ºæ•°æ®è·å–æˆåŠŸ")
                cn_bond['date'] = pd.to_datetime(cn_bond['date'])
                cn_bond = cn_bond.set_index('date')[['close']].rename(columns={'close': 'CN10Y'})
                cn_bond = cn_bond.loc[~cn_bond.index.duplicated(keep='last')]
                cn_bond.sort_index(inplace=True)

            except Exception as e:
                st.warning(f"âš ï¸ ä¸­å€ºæ¥å£æå–å¼‚å¸¸: {e}ï¼Œå›é€€è‡³åŸºå‡†å€¼ 1.8%ï¼Œ2026/02/26")
                cn_bond = pd.DataFrame(index=df_index.index)
                cn_bond['CN10Y'] = 1.8

            # 3. å®è§‚ M1-M2 å‰ªåˆ€å·®
            try:
                st.write("ğŸ” æ­£åœ¨è¿æ¥ï¼šä¸­å›½M1-M2å®è§‚æ•°æ®æº...")
                m1_m2_data = ak.macro_china_money_supply()
                st.success("âœ… M1-M2 æ•°æ®è·å–æˆåŠŸ")
                m1_m2_data['date'] = pd.to_datetime(m1_m2_data.iloc[:, 0].astype(str).str.replace('å¹´', '-').str.replace('æœˆä»½', '-01'))
                m1_m2_data.set_index('date', inplace=True)
                m1_m2_data = m1_m2_data.loc[~m1_m2_data.index.duplicated(keep='last')]
                m1_m2_data.sort_index(inplace=True)
                
                m1_m2_data['M1_M2_Spread'] = m1_m2_data.iloc[:, 5].astype(float) - m1_m2_data.iloc[:, 2].astype(float)
                m1_m2_clean = m1_m2_data[['M1_M2_Spread']]

            except Exception as e:
                st.error(f"âš ï¸ M1-M2 æ•°æ®è·å–å¤±è´¥: {e}")
                st.warning(f"M1-M2 æ¥å£å¾®è°ƒ ({e})ï¼Œå¯ç”¨ -1.2% æ¨¡æ‹Ÿå€¼2026/02/26ï¼Œæˆ–ä½¿ç”¨æ‰‹åŠ¨ä¿®æ”¹")
                m1_m2_clean = -1.2 


            # 4. ä¼°å€¼å› å­ (PEç™¾åˆ†ä½, PB -> å€’æ¨ ROE)
            try:
                # ä½¿ç”¨ funddb æ¥å£è·å–ä¸­è¯å…¨æŒ‡ä¼°å€¼
                st.write("ğŸ” æ­£åœ¨è¿æ¥ï¼šä¸­å›½ä¸­è¯å…¨æŒ‡å®è§‚æ•°æ®æº...")
                df_pe_raw = ak.stock_a_ttm_lyr()
                df_pb_raw = ak.stock_a_all_pb()
                st.success("âœ… ä¸­è¯å…¨æŒ‡ä¼°å€¼æ•°æ®è·å–æˆåŠŸ")
                df_pe_raw['date'] = pd.to_datetime(df_pe_raw['date'])
                df_pe_raw.set_index('date', inplace=True)
                df_pe_raw = df_pe_raw.loc[~df_pe_raw.index.duplicated(keep='last')]
                df_pe_raw.sort_index(inplace=True)
                
                df_pb_raw['date'] = pd.to_datetime(df_pb_raw['date'])
                df_pb_raw.set_index('date', inplace=True)
                df_pb_raw = df_pb_raw.loc[~df_pb_raw.index.duplicated(keep='last')]
                df_pb_raw.sort_index(inplace=True)

                # åˆå¹¶ä¼°å€¼æ•°æ®
                val_df = pd.concat([df_pe_raw[['middlePETTM', 'quantileInRecent10YearsMiddlePeTtm']],df_pb_raw[['middlePB']]], axis=1, join='inner')

                # è®¡ç®—æ ¸å¿ƒå› å­ï¼š
                # 1. PE_Ptile: ä½¿ç”¨æœ€è¿‘10å¹´æ»šåŠ¨å¸‚ç›ˆç‡ä¸­ä½æ•°çš„åˆ†ä½æ•°
                # 2. ROE: åˆ©ç”¨ PB(ä¸­ä½æ•°) / PE(ä¸­ä½æ•°) å€’æ¨å…¨Aè‚¡æ•´ä½“çš„ç›ˆåˆ©èƒ½åŠ›
                val_df['PE_Percentile'] = val_df['quantileInRecent10YearsMiddlePeTtm'].astype(float)
                val_df['ROE'] = val_df['middlePB'].astype(float) / val_df['middlePETTM'].astype(float)
                val_df = val_df[['PE_Percentile', 'ROE']]
                # å¤„ç†æç«¯å€¼æˆ–ç©ºå€¼
                val_df.replace([np.inf, -np.inf], np.nan, inplace=True)

            except Exception as e:
                st.warning(f"ä¼°å€¼æ•°æ®æ‹‰å–å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤å®‰å…¨å€¼50åŠ0.08å¡«å……: {e}")
                val_df = pd.DataFrame(index=df_index.index)
                val_df['PE_Percentile'] = 50.0
                val_df['ROE'] = 0.08

            # 5. å±•ç¤ºä¸“ç”¨å› å­: CPI, PPI
            try:
                st.write("ğŸ” æ­£åœ¨è¿æ¥å…¶ä»–æ•°æ®æº...")
                cpi_df = ak.macro_china_cpi_monthly()
                ppi_df = ak.macro_china_ppi_yearly()
                st.success("âœ… å…¶ä»–æ•°æ®è·å–æˆåŠŸ")

                cpi_val = 0.0
                cpi_date = None
                
                if not cpi_df.empty:
                    i = -1
                    while abs(i) <= len(cpi_df):
                        cpi_val_col2 = cpi_df.iloc[i, 1]
                        cpi_val_col3 = cpi_df.iloc[i, 2]
                        if pd.notna(cpi_val_col3):
                            cpi_val = float(cpi_val_col3) 
                            cpi_date = cpi_val_col2
                            break
                        i -= 1 

                
                ppi_val = 0.0
                ppi_date = None

                if not ppi_df.empty:
                    i = -1
                    while abs(i) <= len(ppi_df):
                        ppi_val_col2 = ppi_df.iloc[i, 1]
                        ppi_val_col3 = ppi_df.iloc[i, 2]
                        if pd.notna(ppi_val_col3):
                            ppi_date = ppi_val_col2
                            ppi_val = float(ppi_val_col3)
                            break
                        i -= 1               
            except Exception as e:
                st.error(f"âš ï¸ å…¶ä»–æ•°æ®æºè·å–å¤±è´¥: {e}")
                st.warning(f"å…¶ä»–æ•°æ®æºæ¥å£å¾®è°ƒ ({e})ï¼Œå¯ç”¨æ¨¡æ‹Ÿå€¼2026/02/26ï¼Œæˆ–ä½¿ç”¨æ‰‹åŠ¨ä¿®æ”¹")
                cpi_val = 0.7
                ppi_val = -2.2
                ppi_date = cpi_date = datetime.datetime.today().strftime('%Y-%m-%d')

            # === æ•°æ®åˆå¹¶ã€å¡«å……ä¸æˆªå– ===
            df = df_index.join([us_bond, cn_bond, m1_m2_clean, val_df[['PE_Percentile', 'ROE']]], how='left')
            df = df.ffill().bfill()

            # ç‰¹å¾å·¥ç¨‹è®¡ç®—
            df['Sino_US_Spread'] = df['CN10Y'].astype(float) - df['DGS10'].astype(float)
            df['Target_5d'] = df['Close'].pct_change(5).shift(-5) # æœªæ¥5æ—¥æ”¶ç›Šç‡
            df['volume'] = np.log1p(df['volume'])

            
            mask = (df.index >= pd.Timestamp(start_date)) & (df.index <= pd.Timestamp(end_date))
            final_df = df.loc[mask]
            status.update(label="âœ… æ¨¡å‹è®­ç»ƒå·²å°±ç»ª", state="complete", expanded=False)

            return final_df, cpi_date, cpi_val,ppi_date,ppi_val

        except Exception as e:
            st.error(f"âš ï¸ æ•°æ®æºåŒæ­¥å¤±è´¥: {e}")
            return pd.DataFrame(), None, 0.0, None, 0.0

# ==========================================
# 3. ä¾§è¾¹æ äº¤äº’é¢æ¿ (å‚æ•°ä¸å¹²é¢„)
# ==========================================
st.sidebar.header("âš™ï¸ åŠ¨æ€å¹²é¢„ä¸æ—¶é—´ç»´åº¦")

default_start = datetime.date.today() - datetime.timedelta(days=1000)
default_end = datetime.date.today()

start_date_obj = st.sidebar.date_input("å¼€å§‹æ—¥æœŸ", value=default_start)
end_date_obj = st.sidebar.date_input("ç»“æŸæ—¥æœŸ", value=default_end)

train_window = st.sidebar.slider("æ»šåŠ¨å›å½’è®­ç»ƒçª—å£ (äº¤æ˜“æ—¥)", 60, 500, 300, 10)
forget_factor = st.sidebar.slider(r"æ—¶é—´é—å¿˜å› å­è¡°å‡åº¦ ($\lambda$)", 0.0, 5.0, 2.0, 0.1, help="å»ºè®® 1.5-2.5,å€¼è¶Šå¤§ï¼Œè¶Šé‡è§†è¿‘æœŸæ•°æ®ã€‚0ä»£è¡¨æ‰€æœ‰å†å²å¹³æƒå¯¹å¾…ã€‚")

st.sidebar.subheader("å¤–éƒ¨å®è§‚å› å­å¹²é¢„ (æƒ…æ™¯æµ‹è¯•)")
use_manual = st.sidebar.checkbox("å¼€å¯æœ€æ–°å› å­æ‰‹åŠ¨å¹²é¢„", value=False)
manual_us10y = st.sidebar.number_input("ç¾å€º10Y (%)", value=4.05, step=0.01)
manual_cn10y = st.sidebar.number_input("ä¸­å€º10Y (%)", value=1.85, step=0.01)
manual_m2 = st.sidebar.number_input("M1-M2 å‰ªåˆ€å·® (%)", value=-1.20, step=0.01, help="M1åŒæ¯” - M2åŒæ¯”")
manual_pe_ptile = st.sidebar.number_input("å…¨A PEç™¾åˆ†ä½ (%)", value=45.0, step=0.1, max_value=100.0,min_value=0.0, help="å½“å‰å…¨Aå¸‚ç›ˆç‡åœ¨å†å²ä¸Šçš„ç™¾åˆ†ä½ä½ç½®ï¼Œè¿‡é«˜å¯èƒ½é¢„ç¤ºä¼°å€¼è¿‡çƒ­")

# ==========================================
# 4. å›å½’è®¡ç®—å¼•æ“ (é—å¿˜å› å­å²­å›å½’)
# ==========================================
if ((end_date_obj - start_date_obj).days >= 30) and start_date_obj >= default_start and end_date_obj <= default_end:
    raw_data, cpi_date, cpi_latest, ppi_date, ppi_latest = fetch_and_align_data(start_date_obj.strftime('%Y%m%d'), end_date_obj.strftime('%Y%m%d'))
    
    if not raw_data.empty:
        df = raw_data.copy()
        
        # 1. äººå·¥å¹²é¢„æœ€æ–°æ•°æ®
        if use_manual:
            df.loc[df.index[-1], 'DGS10'] = manual_us10y
            df.loc[df.index[-1], 'CN10Y'] = manual_cn10y
            df.loc[df.index[-1], 'Sino_US_Spread'] = manual_cn10y - manual_us10y
            df.loc[df.index[-1], 'M1_M2_Spread'] = manual_m2
            df.loc[df.index[-1], 'PE_Percentile'] = manual_pe_ptile

        # 2. å®šä¹‰å›å½’ä¸ƒå¤§å› å­ (åŒ…å«è¡ç”Ÿå› å­)
        # æ³¨: CN10Y æœ¬èº«å·²æ•´åˆè¿› Sino_US_Spread, è¿™é‡Œä¿ç•™ä¸ƒé¡¹æ ¸å¿ƒ
        features = ['Vol_20d', 'DGS10', 'Sino_US_Spread', 'M1_M2_Spread', 'volume', 'PE_Percentile', 'ROE']
        # features = ['Vol_20d', 'DGS10', 'M1_M2_Spread', 'volume', 'ROE']
        # å‰”é™¤æ— æ³•è®­ç»ƒçš„æœ«å°¾5å¤© (è¿™5å¤©åªæœ‰ç‰¹å¾,æ²¡æœ‰Target)
        trainable_df = df.iloc[:-5].dropna(subset=['Target_5d'])
        predict_df = df.iloc[-5:] # æœ€æ–°è¿™5å¤©ï¼Œåªç”¨äºé¢„æµ‹

        corr_matrix = trainable_df[features].corr()
        st.write("### å› å­é—´ç›¸å…³æ€§çŸ©é˜µ (æ£€æŸ¥å…±çº¿æ€§)")
        st.dataframe(corr_matrix.style.background_gradient(cmap='coolwarm'))

        X_train_full = trainable_df[features]
        y_train_full = trainable_df['Target_5d'] 

        predictions, health_scores, actuals, test_dates = [], [], [], []
        
        # 1. é€‰æ‹©æ¨¡å‹
        # model = LassoCV(alphas=np.logspace(-8, -1, 30), cv=5, max_iter=20000)
        # model = ElasticNetCV(l1_ratio=0.5, alphas=np.logspace(-8, -1, 30), cv=5)
        model = RidgeCV(alphas=np.logspace(-2, 2, 20),cv=5)        
        
        # 3. æ»šåŠ¨è®­ç»ƒ (æŒ‡æ•°è¡°å‡æ ·æœ¬æƒé‡)
        progress_text = "æ­£åœ¨æ‰§è¡Œå¸¦é—å¿˜å› å­çš„æ»šåŠ¨å›å½’..."
        my_bar = st.progress(0, text=progress_text)
        total_steps = len(trainable_df) - train_window
        
        for idx, i in enumerate(range(train_window, len(trainable_df))):
            # è·å–å½“å‰çª—å£æ•°æ®
            X_win = X_train_full.iloc[i-train_window : i]
            y_win = y_train_full.iloc[i-train_window : i]
            # y_win = y_win * 100 # æ”¾å¤§ç›®æ ‡å˜é‡ï¼Œå¢å¼ºæ¨¡å‹å¯¹å¾®å°æ”¶ç›Šç‡çš„æ•æ„Ÿåº¦

            # æ„é€ é—å¿˜æƒé‡ (Exponential Decay Weight)
            weights = np.exp(np.linspace(-forget_factor, 0, train_window))
            
            scaler = StandardScaler()
            X_win_scaled = scaler.fit_transform(X_win)
            if i == train_window:
                # æ£€æŸ¥æ˜¯å¦æœ‰å› å­çš„æ–¹å·®ä¸º 0
                print("å„å› å­æ ‡å‡†å·®:\n", X_win.std()) 
                # æ£€æŸ¥ y æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ³¢åŠ¨
                print("æ”¶ç›Šç‡æ ‡å‡†å·®:", y_win.std())
            
            model.fit(X_win_scaled, y_win, sample_weight=weights)
            
            # é¢„æµ‹ç¬¬ i å¤©
            X_curr = scaler.transform(X_train_full.iloc[i:i+1])
            pred_return = model.predict(X_curr)[0]
            curr_pe_ptile = X_train_full['PE_Percentile'].iloc[i]
            
            # --- å¥åº·åˆ†é‡æ„é€»è¾‘ ---
            # åŸºç¡€è¯„åˆ†ï¼šæ¨¡å‹é¢„æµ‹æœŸæœ›æ”¶ç›Šæ˜ å°„
            base_score = 50 + (pred_return * 1000)
            
            # è¿‡çƒ­æƒ©ç½šï¼šå½“å…¨ A PEç™¾åˆ†ä½å¤§äº70%æ—¶ï¼Œæ¯è¶…1%æ‰£é™¤0.5åˆ†ï¼›å¤§äº90%æ¯è¶…1%æ‰£é™¤1.5åˆ†
            penalty = 0
            if curr_pe_ptile > 90:
                penalty = (90 - 70) * 0.5 + (curr_pe_ptile - 90) * 1.5
            elif curr_pe_ptile > 70:
                penalty = (curr_pe_ptile - 70) * 0.5
                
            score = base_score - penalty
            
            health_scores.append(np.clip(score, 0, 100))
            predictions.append(pred_return)
            actuals.append(y_win.iloc[-1]) # å¯¹åº”çš„å®é™…æœªæ¥5æ—¥æ”¶ç›Š
            test_dates.append(trainable_df.index[i])
            
            if idx % 50 == 0:
                my_bar.progress(idx / total_steps, text=progress_text)

        print("å› å­é¢„æµ‹åŠ›è¯Šæ–­ (IC):")
        print(trainable_df[features].corrwith(trainable_df['Target_5d']))
        # åœ¨å¾ªç¯æœ€å
        coeffs = dict(zip(features, model.coef_))
        print(f"å½“å‰ç³»æ•°åˆ†é…: {coeffs}")
        model.alpha_        
        my_bar.empty()
        
        # 4. é¢„æµ‹æœ€å 5 å¤©ä¸æœªæ¥ 5 å¤©
        future_dates = [] #list(predict_df.index)
        future_preds = []
        future_scores = []
        future_prices = []
        
        last_actual_price = df['Close'].iloc[-6] # æœ€åä¸€ä¸ªæœ‰å®Œæ•´ 5 æ—¥æ”¶ç›Šçš„æ•°æ®ç‚¹çš„ä»·æ ¼

        # (A) é¢„æµ‹ç¼ºå¤±çš„æœ€è¿‘ 5 å¤©
        for i in range(len(predict_df)):
                        
            curr_row = predict_df.iloc[i:i+1]
            curr_date = predict_df.index[i]
            # è¿™5å¤©è™½ç„¶æ²¡æœ‰Targetï¼Œä½†ç‰¹å¾æ˜¯å·²çŸ¥çš„ï¼Œä¸ç®—çœŸæ­£çš„å¤–æ¨           
            # é¢„æµ‹ 5 æ—¥æ”¶ç›Šç‡            
            curr_x = scaler.transform(curr_row[features])
            pred_ret_5d = model.predict(curr_x)[0]

            # è®¡ç®—é¢„æµ‹ç‚¹ä½ (åŸºäº drift æ¼‚ç§»)
            daily_drift = pred_ret_5d / 5
            pred_price = last_actual_price * (1 + daily_drift * (i + 1))

            pe_ptile = predict_df['PE_Percentile'].iloc[i]
            base_score = 50 + (pred_ret_5d * 1000)
            penalty = max(0, pe_ptile - 70) * 0.5 if pe_ptile <= 90 else (10 + (pe_ptile - 90) * 1.5)
            
            future_dates.append(curr_date)
            future_prices.append(pred_price)
            future_preds.append(pred_ret_5d)
            future_scores.append(np.clip(base_score - penalty, 0, 100))


        # (B) å¤–æ¨é¢„æµ‹æœªæ¥ 5 å¤© (å‡è®¾å½“å‰å®è§‚å› å­ä¿æŒä¸å˜ï¼Œä»…æ—¶é—´åç§»)
        last_actual_price = predict_df['Close'].iloc[-1] # å¤–æ¨èµ·ç‚¹ä»·æ ¼ä¸ºæœ€åä¸€ä¸ªå·²çŸ¥ä»·æ ¼
        last_known_features = predict_df.iloc[-1:]
        
        future_to_predict_days = 5
        step_count = 0

        for i in range(1, 10):# å‘¨æœ«è·³è¿‡ï¼Œå®é™…å¤–æ¨7å¤©ï¼Œçº¦ç­‰äº5ä¸ªäº¤æ˜“æ—¥
            
            if step_count >= future_to_predict_days:break
            curr_date = predict_df.index[-1] + pd.Timedelta(days=i)
            if curr_date.weekday() >= 5: continue # è·³è¿‡å‘¨æœ«
            step_count += 1
            # ä½¿ç”¨æœ€æ–°çš„ç‰¹å¾æ¨æ¼”æœªæ¥
            curr_date = last_known_features.index[0] + pd.Timedelta(days=i)
            # é¢„æµ‹ 5 æ—¥æ”¶ç›Šç‡
            curr_x = scaler.transform(last_known_features[features])
            pred_ret_5d = model.predict(curr_x)[0] # è¿™é‡Œçš„ pred_ret_5d æ˜¯åŸºäºæœ€åå·²çŸ¥ç‰¹å¾çš„æœªæ¥5æ—¥æ”¶ç›Šé¢„æµ‹ï¼Œè™½ç„¶ç‰¹å¾ä¸å˜ï¼Œä½†æ—¶é—´æ¨è¿›ä¼šå½±å“é¢„æµ‹ç»“æœï¼ˆå¦‚æœæ¨¡å‹å¯¹æ—¶é—´æ•æ„Ÿçš„è¯ï¼‰ã€‚
            
            # è®¡ç®—é¢„æµ‹ç‚¹ä½ (åŸºäº drift æ¼‚ç§»)
            daily_drift = pred_ret_5d / 5
            pred_price = last_actual_price * (1 + daily_drift * (step_count))
            
            pe_ptile = last_known_features['PE_Percentile'].iloc[0]
            base_score = 50 + (pred_ret_5d * 1000)
            penalty = max(0, pe_ptile - 70) * 0.5 if pe_ptile <= 90 else (10 + (pe_ptile - 90) * 1.5)
            
            noise = np.random.normal(0, 2)# ç»™æœªæ¥çš„å¥åº·åˆ†å¢åŠ ä¸€å®šçš„ä¸ç¡®å®šæ€§æŠ–åŠ¨ (æ¨¡æ‹Ÿéšæœºæ¸¸èµ°)
            final_score = np.clip(base_score - penalty + noise, 0, 100)

            future_dates.append(curr_date)
            future_prices.append(pred_price)
            future_preds.append(pred_ret_5d)
            future_scores.append(final_score)
        
        # ==========================================
        # 5. å¯è§†åŒ–ä¸å±•ç¤º
        # ==========================================
        res_df = pd.DataFrame({'Close': df.loc[test_dates, 'Close'], 'Health_Score': health_scores}, index=test_dates)
        future_df = pd.DataFrame({'Future_Score': future_scores, 'Expected_Ret': future_preds, 'Expected_Price': future_prices}, index=future_dates)
        
        st.header("å®æ—¶ç›‘æµ‹æŒ‡æ ‡å¤§å±" + f" (æˆªè‡³ {future_df.index[-1].strftime('%Y-%m-%d')})")
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        col1.metric("ä¸­è¯å…¨æŒ‡å¸‚ç›ˆç‡(PE)åˆ†ä½", f"{df['PE_Percentile'].iloc[-1]*100:.1f}%", delta="ä¼°å€¼è¿‡çƒ­è­¦æŠ¥" if df['PE_Percentile'].iloc[-1] *100> 75 else "ä¼°å€¼å¥åº·", delta_color="inverse")
        col2.metric("ä¸­å…¨æŒ‡ä¸­ä½ROE", f"{df['ROE'].iloc[-1]*100:.2f}%")
        col3.metric("M1_M2_å‰ªåˆ€å·®", f"{df['M1_M2_Spread'].iloc[-1]:.2f}%")
        col4.metric(f"{cpi_date}CPIåŒæ¯”", f"{cpi_latest}%")
        col5.metric(f"{ppi_date}PPIåŒæ¯”", f"{ppi_latest}%")
        col6.metric("5æ—¥æœªæ¥é¢„æµ‹ä»·æ ¼", f"{future_df['Expected_Price'].iloc[-1]:.2f}")
        
        tab1, tab2, tab3 = st.tabs(["ğŸ“Š å®è§‚-ä¼°å€¼ç»¼åˆå¥åº·è¿½è¸ªä¸é¢„æµ‹", "ğŸ² å¸ƒæœ—è¿åŠ¨è·¯å¾„æ¨æ¼”", "ğŸ“¥ å› å­åº“åº•ç¨¿ä¸‹è½½"])
        
        with tab1:
            st.subheader("æŒ‡æ•°èµ°åŠ¿ä¸ç»¼åˆå¥åº·è¯„åˆ† (å«è¿‘/è¿œæœŸé¢„æµ‹)")
            
            fig = make_subplots(specs=[[{"secondary_y": True}]])
            
            # å†å²èµ°åŠ¿ä¸è¯„åˆ†
            fig.add_trace(go.Scatter(x=res_df.index, y=res_df['Close'], name='ä¸Šè¯æŒ‡æ•°', line=dict(color='red', width=2)), secondary_y=False)
            fig.add_trace(go.Scatter(x=res_df.index, y=res_df['Health_Score'], name='å†å²å¥åº·è¯„åˆ†', fill='tozeroy', fillcolor='rgba(0,176,246,0.3)', line=dict(color='rgba(0,176,246,1)')), secondary_y=True)
            
            #æœªæ¥èµ°åŠ¿
            # 1. é¢„æµ‹ç‚¹ä½ (çº¢è‰²è™šçº¿)
            fig.add_trace(go.Scatter(x=future_df.index, y=future_df['Expected_Price'], name='SSE é¢„æµ‹è·¯å¾„', line=dict(color='red', width=2, dash='dash')), secondary_y=False)
        
            # é¢„æµ‹åŒºåŸŸ (æœ€è¿‘ç¼ºå¤±5å¤© + æœªæ¥30å¤©)
            fig.add_trace(go.Scatter(x=future_df.index, y=future_df['Future_Score'], name='æ¨¡å‹å¤–æ¨é¢„æµ‹è¯„åˆ† (å«ä¸ç¡®å®šæ€§)', line=dict(color='orange', width=2, dash='dot')), secondary_y=True)
            # 2. é¢„æµ‹å¥åº·åˆ† (æ©™è‰²å®çº¿)
            fig.add_trace(go.Scatter(x=future_df.index, y=future_df['Future_Score'], name='é¢„æµ‹å¥åº·åˆ†', line=dict(color='orange', width=2)), secondary_y=True)

            # è­¦æˆ’çº¿
            fig.add_hline(y=75, line_dash="dash", line_color="orange",annotation_text="è¿‡çƒ­çº¿ (75)", annotation_position="bottom right", secondary_y=True)
            fig.add_hline(y=50, line_dash="solid", line_color="gray", annotation_text="å‡è¡¡ä¸­æ¢ (50)",annotation_position="bottom right", secondary_y=True)
            fig.add_hline(y=40, line_dash="dash", line_color="green", annotation_text="ä½ä¼°çº¿ (40)",annotation_position="bottom right", secondary_y=True)

            fig.update_layout(height=600, hovermode="x unified", title="èåˆä¼°å€¼æƒ©ç½šçš„é—å¿˜å› å­åŠ¨æ€å›å½’", legend=dict(orientation="h", y=1.05), margin=dict(l=50, r=50, t=50, b=50))
            st.plotly_chart(fig, width='stretch')
            
        sim_days = 60
        with tab2:
            st.subheader(f"åŸºäºæœ€æ–°å› å­çš„å¸ƒæœ—è¿åŠ¨æ¨¡æ‹Ÿ (æœªæ¥ {sim_days} å¤©)")
            latest_close = df['Close'].iloc[-1]
            latest_score = res_df['Health_Score'].iloc[-1]
            
            # è®¡ç®—é¢„æœŸæ¼‚ç§»ä¸æ³¢åŠ¨
            recent_vol = df['Return'].tail(60).std() * np.sqrt(252)
            drift_adj = (latest_score - 50) / 100 * 0.1 
            mu = df['Return'].mean() * 252 + drift_adj
            
            dt = 1/252
            sim_paths = 2000
            paths = np.zeros((sim_days, sim_paths))
            paths[0] = latest_close
            
            for t in range(1, sim_days):
                paths[t] = paths[t-1] * np.exp((mu - 0.5 * recent_vol**2)*dt + recent_vol * np.sqrt(dt) * np.random.standard_normal(sim_paths))
            
            fig_mc = go.Figure()
            fig_mc = go.Figure()
            # ç”»å‰ 100 æ¡è·¯å¾„ä½œä¸ºå±•ç¤º
            for i in range(min(100, sim_paths)):
                fig_mc.add_trace(go.Scatter(y=paths[:, i], mode='lines', line=dict(color='gray', width=1), opacity=0.1, showlegend=False))
            
            p_5, p_50, p_95 = np.percentile(paths, 5, axis=1), np.percentile(paths, 50, axis=1), np.percentile(paths, 95, axis=1)
            
            fig_mc.add_trace(go.Scatter(y=p_95, mode='lines', name='95% ä¹è§‚è¾¹ç•Œ', line=dict(color='red', dash='dash')))
            fig_mc.add_trace(go.Scatter(y=p_50, mode='lines', name='50% ç¨³å¥ä¸­æ¢', line=dict(color='blue', width=3)))
            fig_mc.add_trace(go.Scatter(y=p_5, mode='lines', name='5% æ‚²è§‚æ”¯æ’‘', line=dict(color='green', dash='dash')))
            fig_mc.update_layout(height=450, title="æœªæ¥æŒ‡æ•°å¯èƒ½è¿è¡Œè·¯å¾„ä¸æ¦‚ç‡åŒºé—´", xaxis_title="æœªæ¥äº¤æ˜“å¤©æ•°", yaxis_title="æŒ‡æ•°ç‚¹ä½")
            st.plotly_chart(fig_mc, width='stretch')
            
            st.success(f"**æ¨¡æ‹Ÿæµ‹ç®—ç»“è®ºï¼š** æœªæ¥ {sim_days} å¤©åï¼Œå¤§æ¦‚ç‡ï¼ˆ90%ç½®ä¿¡åº¦ï¼‰è½åœ¨ **{p_5[-1]:.0f} ç‚¹** åˆ° **{p_95[-1]:.0f} ç‚¹** ä¹‹é—´ï¼Œä¸­æ¢ç›®æ ‡ä½ **{p_50[-1]:.0f} ç‚¹**ã€‚")

        with tab3:
            st.write(f"**æ¨¡å‹å†å²é¢„æµ‹ IC å€¼:** `{pd.Series(predictions).corr(pd.Series(actuals)):.4f}` *(æ³¨ï¼šåœ¨é‡åŒ–å¤šå› å­ä¸­ï¼ŒICå€¼ç»å¯¹å€¼ > 0.03 å³è¢«è®¤ä¸ºå…·æœ‰æœ‰æ•ˆé¢„æµ‹èƒ½åŠ›> 0.1: å±äºæå¼ºçš„é¢„æµ‹å› å­ï¼Œéå¸¸ç½•è§ã€‚è¶Šæ¥è¿‘ 1: é¢„æµ‹è¶Šå‡†ï¼›è¶Šæ¥è¿‘ -1: é¢„æµ‹è¶Šåå‘ï¼ˆä¹Ÿå¯ä»¥ç”¨ï¼‰ï¼›æ¥è¿‘ 0: æ¨¡å‹åœ¨ççŒœã€‚)*")
            st.dataframe(df.tail(100).sort_index(ascending=False), width='stretch')
            
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='Factor_Database')
                res_df.to_excel(writer, sheet_name='Historical_Scores')
                future_df.to_excel(writer, sheet_name='Future_Prediction')
            
            st.download_button("ğŸ“¥ å¯¼å‡ºé‡åŒ–è¿½è¸ª Excel", output.getvalue(), f"Quant_Report_{datetime.date.today()}.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
else:
    st.warning("è¯·é€‰æ‹©æœ‰æ•ˆçš„æ—¥æœŸåŒºé—´å¼€å§‹åˆ†æã€‚")